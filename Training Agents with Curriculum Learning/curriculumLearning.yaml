# sample config file for tanks tutorial
# NOTE:  YAML files are sensitive to indents!  wrong indent level can mess things up!

# engine settings below are global to training session, kind of like using command line arguments
# these below are like having defaults that can be overridden by command line arguments
engine_settings:
  time_scale: 90.0  # equivalent to --time-scale=90.0
  no_graphics: true  # equivalent to --no-graphics

  # REMEMBER if using visual observations, you CANNOT use --no-graphics

behaviors:
   BotKiller: #3DBall:
    trainer_type: ppo
    summary_freq: 10000
    # below are hyperparameter settings
    # settings below are defaults, just included here to make it easy for you to tweak them
    # hyperparameters:
    #   batch_size: 1024
    #   buffer_size: 10240
    #   learning_rate: 3.0e-4
    #   learning_rate_schedule: linear
    #   # parameters specific to PPO
    #   beta: 5.0e-3
    #   epsilon: 0.2
    #   lambd: 0.95
    #   num_epoch: 3

    max_steps: 5.0e6  # 1M total steps of training
    time_horizon: 200  # using agent 1000 max steps per episode, with decision frequency 5, so 200 experiences
    reward_signals:  # SimpleTank reward types
      extrinsic:  # "normal" rewards
        strength: 1.0
        gamma: 0.9

      # gail:  # GAIL Settings (uncomment below to add it in)
      #   # strength: 0.08
      #   # gamma: 0.9
      #   demo_path: .\demonstrations\discrete-1  # set this to be your /path/to/demo/files
      #   use_actions: true

      # for demo_path above, this is relative to where you are calling mlagents-learn command from
      # if you are at C:\users\lance\tanks, and running mlagents-learn there,
      # and if you have a folder called C:\users\lance\tanks\demos\  which has demo files in there,
      # you can set demo_path to be  demo_path: .\demos

      # curiosity:  # Curiosity Settings (uncomment below to add it in)
      #   strength: 0.02
      #   gamma: 0.9
      #   encoding_size: 64
      #   learning_rate: 3.0e-4


   BotKillerRaycast:   #3DBall:  
    trainer_type: ppo
    summary_freq: 10000
    # below are hyperparameter settings
    # settings below are defaults, just included here to make it easy for you to tweak them
    # hyperparameters:
    #   batch_size: 1024
    #   buffer_size: 10240
    #   learning_rate: 3.0e-4
    #   learning_rate_schedule: linear
    #   # parameters specific to PPO
    #   beta: 5.0e-3
    #   epsilon: 0.2
    #   lambd: 0.95
    #   num_epoch: 3

    max_steps: 5.0e6  # 1M total steps of training
    time_horizon: 200  # using agent 1000 max steps per episode, with decision frequency 5, so 200 experiences
    reward_signals:  # SimpleTank reward types
      extrinsic:  # "normal" rewards
        strength: 1.0
        gamma: 0.9

      # gail:  # GAIL Settings (uncomment below to add it in)
      #   # strength: 0.08
      #   # gamma: 0.9
      #   demo_path: .\demonstrations\discrete-1  # set this to be your /path/to/demo/files
      #   use_actions: true

      # for demo_path above, this is relative to where you are calling mlagents-learn command from
      # if you are at C:\users\lance\tanks, and running mlagents-learn there,
      # and if you have a folder called C:\users\lance\tanks\demos\  which has demo files in there,
      # you can set demo_path to be  demo_path: .\demos

      # curiosity:  # Curiosity Settings (uncomment below to add it in)
      #   strength: 0.02
      #   gamma: 0.9
      #   encoding_size: 64
      #   learning_rate: 3.0e-4

environment_parameters:
    level:
        curriculum:
            - name: Lesson0
              completion_criteria:
                measure: reward
                behavior: BotKillerRaycast
                signal_smoothing: true
                min_lesson_length: 1
                threshold: 0.2 #need to kill 1 enemy
              value: 0.0 #nextLevel is level1
            - name: Lesson1
              completion_criteria:
                measure: reward
                behavior: BotKillerRaycast
                signal_smoothing: true
                min_lesson_length: 1
                threshold: 0.8 #need to kill 2 enemies
              value: 1.0 #nextLevel is level2
            - name: Lesson2
              completion_criteria:
                measure: reward
                behavior: BotKillerRaycast
                signal_smoothing: true
                min_lesson_length: 1
                threshold: 2.2 #need to kill 3 enemies
              value: 2.0 #nextLevel is level3
            - name: Lesson3
              completion_criteria:
                measure: reward
                behavior: BotKillerRaycast
                signal_smoothing: true
                min_lesson_length: 1
                threshold: 2.75 #need to kill 4 enemies
              value: 3.0 #nextLevel is going to stay at level3
